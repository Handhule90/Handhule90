{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkIa4R87KfqhNL2EVnc0OQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Handhule90/Handhule90/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ownHzufY4B3K",
        "outputId": "a0855606-c6d9-48ad-95a7-f413ae88dc18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.12/dist-packages (2025.11.0)\n",
            "Collecting netCDF4\n",
            "  Downloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: packaging>=24.1 in /usr/local/lib/python3.12/dist-packages (from xarray) (25.0)\n",
            "Collecting cftime (from netCDF4)\n",
            "  Downloading cftime-1.6.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading netcdf4-1.7.3-cp311-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cftime-1.6.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cftime, netCDF4\n",
            "Successfully installed cftime-1.6.5 netCDF4-1.7.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas requests xarray netCDF4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "\n",
        "# Base URL of the website\n",
        "BASE_URL = \"https://ncics.org/ibtracs/index.php\"\n",
        "BASE_URL_ALT = \"https://ncics.org/ibtracs/\"\n",
        "\n",
        "BASIN_ABBREVIATIONS = {\n",
        "    \"Northern Atlantic\": \"na\",\n",
        "    \"Eastern Pacific\" : \"ep\",\n",
        "    \"Western Pacific\": \"wp\",\n",
        "    \"Northern Indian\": \"ni\",\n",
        "    \"Southern Indian\": \"si\",\n",
        "    \"Southern Pacific\": \"sp\"\n",
        "}\n",
        "\n",
        "def fetch_year_page(year):\n",
        "    url = f\"{BASE_URL}?name=YearBasin-{year}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve page for year {year}: {response.status_code}\")\n",
        "        return None\n",
        "    return response.text\n",
        "\n",
        "def extract_links_from_second_table(html):\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "    tables = soup.find_all('table', {'class': 'ishade', 'summary': 'Layout table.'})\n",
        "    if len(tables) < 2:\n",
        "        print(\"Less than two tables found on the page.\")\n",
        "        return None\n",
        "    table = tables[1]\n",
        "    headers = table.find('tr').find_all('td')\n",
        "    basins = [header.text.strip() for header in headers]\n",
        "    basin_links = {basin: [] for basin in basins}\n",
        "    rows = table.find_all('tr')[1:]\n",
        "    for row in rows:\n",
        "        cells = row.find_all('td')\n",
        "        for index, cell in enumerate(cells):\n",
        "            links = cell.find_all('a', href=True)\n",
        "            for link in links:\n",
        "                basin_links[basins[index]].append(f\"{BASE_URL_ALT}{link['href']}\")\n",
        "    return basin_links\n",
        "\n",
        "def scrape_fourth_table(link):\n",
        "    response = requests.get(link)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve page: {response.status_code}\")\n",
        "        return None\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    tables = soup.find_all('table')\n",
        "    if len(tables) < 4:\n",
        "        print(\"Less than four tables found on the page.\")\n",
        "        return None\n",
        "    table = tables[3]\n",
        "    rows = table.find_all('tr')\n",
        "    table_data = []\n",
        "    for row in rows[2:]:\n",
        "        cells = row.find_all(['td', 'th'])\n",
        "        row_data = [cell.text.strip() for cell in cells]\n",
        "        table_data.append(row_data)\n",
        "    return table_data\n",
        "\n",
        "def add_missing_dates_and_empty_cells(data):\n",
        "    last_date = None\n",
        "    last_row = None\n",
        "    if data:\n",
        "        data[0] = ['N / A' if not cell else cell for cell in data[0]]\n",
        "    for row in data:\n",
        "        datetime_cell = row[1]\n",
        "        if \" \" in datetime_cell:\n",
        "            last_date = datetime_cell.split()[0]\n",
        "        else:\n",
        "            row[1] = f\"{last_date} {datetime_cell}\" if last_date else datetime_cell\n",
        "        for i, cell in enumerate(row):\n",
        "            if cell == \"N / A\":\n",
        "                row[i] = None\n",
        "            elif not cell and last_row:\n",
        "                row[i] = last_row[i]\n",
        "        last_row = row\n",
        "    return data\n",
        "\n",
        "def get_typhoon_name_from_link(link):\n",
        "    response = requests.get(link)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        name_element = soup.find('h1')\n",
        "        if name_element:\n",
        "            return name_element.get_text(strip=True)\n",
        "    return None\n",
        "\n",
        "def save_cache(data, year, basin_name, folder_path=\"data\"):\n",
        "    basin_abbr = BASIN_ABBREVIATIONS.get(basin_name, \"unknown\")\n",
        "    if not os.path.exists(folder_path):\n",
        "        os.makedirs(folder_path)\n",
        "    cache_file = os.path.join(folder_path, f\"{basin_abbr}_{year}_data.json\")\n",
        "    with open(cache_file, \"w\") as file:\n",
        "        json.dump(data, file, indent=4)\n",
        "    print(f\"Data cached to file: {cache_file}\")\n",
        "\n",
        "def load_cache(year, basin_name, folder_path=\"data\"):\n",
        "    basin_abbr = BASIN_ABBREVIATIONS.get(basin_name, \"unknown\")\n",
        "    cache_file = os.path.join(folder_path, f\"{basin_abbr}_{year}_data.json\")\n",
        "    if os.path.exists(cache_file):\n",
        "        with open(cache_file, \"r\") as file:\n",
        "            try:\n",
        "                data = json.load(file)\n",
        "                print(f\"Loaded data from cache: {cache_file}\")\n",
        "                return data\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Error loading cache file {cache_file}. Scraping new data.\")\n",
        "                return None\n",
        "    return None\n",
        "\n",
        "def save_data_as_json(year, basin_name, month=None, folder_path=\"data\"):\n",
        "    links_by_basin = extract_links_from_second_table(fetch_year_page(year))\n",
        "    if not links_by_basin:\n",
        "        return None\n",
        "    if basin_name not in links_by_basin:\n",
        "        print(f\"Basin '{basin_name}' not found. Available basins: {list(links_by_basin.keys())}\")\n",
        "        return None\n",
        "    all_typhoon_data = []\n",
        "    for link in links_by_basin[basin_name]:\n",
        "        typhoon_name = get_typhoon_name_from_link(link)\n",
        "        if not typhoon_name:\n",
        "            continue\n",
        "        composite_name = typhoon_name.split()\n",
        "        if len(composite_name) >= 2:\n",
        "            typhoon_name = composite_name[-2]\n",
        "        else:\n",
        "            typhoon_name = \"UNKNOWN\"\n",
        "        fourth_table_data = scrape_fourth_table(link)\n",
        "        if not fourth_table_data:\n",
        "            continue\n",
        "        processed_data = add_missing_dates_and_empty_cells(fourth_table_data)\n",
        "        typhoon_data = {\"name\": typhoon_name, \"path\": []}\n",
        "        for row in processed_data:\n",
        "            time = row[1]\n",
        "            if time:\n",
        "                try:\n",
        "                    time_obj = datetime.strptime(time, \"%Y-%m-%d %H:%M:%S\")\n",
        "                    if month and time_obj.month != int(month):\n",
        "                        continue  # Skip rows outside the requested month\n",
        "                    time = time_obj.strftime(\"%Y-%m-%d %H:%M\")\n",
        "                except ValueError:\n",
        "                    pass\n",
        "            lat = row[3] if row[3] != \"N / A\" else None\n",
        "            long = row[4] if row[4] != \"N / A\" else None\n",
        "            speed = row[5] if row[5] != \"N / A\" else None\n",
        "            pressure = row[6] if row[6] != \"N / A\" else None\n",
        "            if speed:\n",
        "                speed = int(speed)\n",
        "                if speed < 34:\n",
        "                    typhoon_class = 0\n",
        "                elif 34 <= speed <= 63:\n",
        "                    typhoon_class = 1\n",
        "                elif 64 <= speed <= 82:\n",
        "                    typhoon_class = 2\n",
        "                elif 83 <= speed <= 95:\n",
        "                    typhoon_class = 3\n",
        "                elif 96 <= speed <= 112:\n",
        "                    typhoon_class = 4\n",
        "                elif speed >= 113:\n",
        "                    typhoon_class = 5\n",
        "            else:\n",
        "                typhoon_class = 0\n",
        "            typhoon_data[\"path\"].append({\n",
        "                \"time\": time,\n",
        "                \"lat\": float(lat) if lat else None,\n",
        "                \"long\": float(long) if long else None,\n",
        "                \"speed\": str(speed) if speed else \"< 35\",\n",
        "                \"pressure\": str(pressure) if pressure else \"> 1008\",\n",
        "                \"class\": typhoon_class\n",
        "            })\n",
        "        if processed_data:\n",
        "            try:\n",
        "                start_time = datetime.strptime(processed_data[0][1], \"%Y-%m-%d %H:%M:%S\")\n",
        "                start_time = start_time.replace(second=0)\n",
        "                typhoon_data[\"start_time\"] = int(start_time.timestamp())\n",
        "            except ValueError:\n",
        "                typhoon_data[\"start_time\"] = None\n",
        "        all_typhoon_data.append(typhoon_data)\n",
        "    save_cache(all_typhoon_data, year, basin_name, folder_path)\n",
        "    return all_typhoon_data\n",
        "\n",
        "def scrape_typhoon_data(year, basin_name, month=None, folder_path=\"data\"):\n",
        "    data = load_cache(year, basin_name, folder_path)\n",
        "    if data:\n",
        "        return data\n",
        "    else:\n",
        "        print(f\"Cache not found. Scraping data for {basin_name} in {year}.\")\n",
        "        return save_data_as_json(year, basin_name, month, folder_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    year = input(\"Enter year (e.g., 2025): \").strip()\n",
        "    month = input(\"Enter month (1-12, optional, press Enter to skip): \").strip() or None\n",
        "    basin = input(\"Enter basin (e.g., Western Pacific): \").strip()\n",
        "    data = scrape_typhoon_data(year, basin, month)\n",
        "    if data:\n",
        "        print(f\"Fetched and cached {len(data)} typhoons.\")\n",
        "    else:\n",
        "        print(\"No data available.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUy4o_H49Oru",
        "outputId": "40b361e7-a360-4f00-941a-bd8918b5de42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter year (e.g., 2025): 2025\n",
            "Enter month (1-12, optional, press Enter to skip): \n",
            "Enter basin (e.g., Western Pacific): Western Pacific\n",
            "Cache not found. Scraping data for Western Pacific in 2025.\n"
          ]
        }
      ]
    }
  ]
}